---
title: "explore"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{explore}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE,
  echo = TRUE
)
```

```{r setup, eval=TRUE}
library(capstone)
library(tidyverse)
library(tidytext)
raw_data_dir<-paste0(here::here(),"/../capstone raw data/")
en_data_dir<-paste0(raw_data_dir,"final/en_US")
board_folder=glue::glue("{Sys.getenv('PIN_LOCAL_FOLDER')}capstone")
board <- pins::board_folder(board_folder,versioned = TRUE)
```

## Loading helper function

Lets just sample the files for about 30% of their content to work with them

```{r eval=FALSE}
random_chunk_reader<-function(x,pos){
    tibble(
        text=sample(x,length(x)*0.33)
    )
}
sampled_files<-list.files(en_data_dir)%>%
    purrr::map_df(.,function(fname){
        tibble(file=fname,
               content=list(readr::read_lines_chunked(paste0(en_data_dir,"/",fname),
                                  callback = DataFrameCallback$new(random_chunk_reader))
               ))
    })%>%
  unnest(cols=c(content))%>%
  group_by(file)%>%
  mutate(rnum=row_number())%>%
  ungroup()%>%
   mutate(
        from=str_remove(file,"en_US\\."),
        from=str_remove(from,"\\.txt")
    )
```

## Exploring with tidytext

Im basing everything on [Julia Silges new book](https://www.tidytextmining.com/).

### Tokenification

```{r}

tok_corpora<-sampled_files%>%
  unnest_tokens( input = "text", output = "word" )%>%
  anti_join(stop_words)
  
```

Lets save in pins what we have for now:

```{r }

write_sampled_and_tokenized <- function() {
pins::pin_write(board = board,
                x = sampled_files,
                name = "sampled_files",
                description = "Coursera capstone 33% sampled files")

pins::pin_write(board = board,
                x = tok_corpora,
                name = "tok_corpora",
                description = "Coursera capstone tokenized no stopwords")
}

read_sampled_and_tokenized <- function() {
  
  sampled_files <<- pins::pin_read(board = board,
                                   name = "sampled_files")
  tok_corpora <<- pins::pin_read(board = board,
                                   name = "tok_corpora")
  
}

```

Lets see what we've got:

```{r}
top10_words<-tok_corpora%>%
    group_by(file)%>%
    count(word,sort=TRUE)%>%
    slice_max(order_by=n,n=10)

top10_words
```

Now lets look at the top 10:

```{r}
top10_words%>%
    mutate(
        from=str_remove(file,"en_US\\."),
        from=str_remove(from,"\\.txt")
    )%>%
    group_by(from,word)%>%
    mutate(word=fct_reorder(word,n))%>%
    ggplot(aes(x=word,y=n))+
    geom_col()+
    facet_wrap(.~from)+
    coord_flip()
```

Well that is poor. Words that are just integers, rt and follow do not seem like that interesting to see whats up.

```{r}
top10_words<-tok_corpora%>%
    group_by(file)%>%
    filter(!str_detect(word,"^[0-9]+$|^it’s$|^don’t$|^i’m$|rt|lol") )%>%
    count(word,sort=TRUE)%>%
    slice_max(order_by=n,n=10)%>%
  mutate(
        from=str_remove(file,"en_US\\."),
        from=str_remove(from,"\\.txt")
    )

```

```{r}
top10_words%>%
    group_by(from)%>%
    mutate(word=fct_reorder(word,n))%>%
    ggplot(aes(x=word,y=n))+
    geom_col()+
    facet_wrap(.~from)+
    coord_flip()+
  theme(axis.text.x = element_text(angle = -30, vjust = 1, hjust = 0))
```

Now that's more likely. I like this plot because it lets us see some intersections between the top 10 most frequent words in our three source datasets.

Furthermore, word frequency analysis for the purpose of cleaning by using the next code and visually exploring the resulting table:

```{r}
tok_table<-tok_corpora%>%
    count(from,word,sort=TRUE,name = "word_count")
tok_table%>%
    group_by(from)%>%
    arrange(word_count)%>%
  knitr::kable()
```

This shows us that for the three origins there are a bunch of underscores. Sometimes just as lines, sometimes surrounding words.

Also, there are many long strings of numbers. This requires a set of cleaning functions akin to those seen in the top10 analysis. This needs to be more thorough perhaps even applying it to the base sampled_files corpora, then tokenizing again, and then proceed with ngram analysis.

```{r}

sampled_files<-sampled_files%>%
    mutate(text=str_remove_all(text,"^_+$|^[0-9\\._]{2,}"))%>%
    filter(!(text==""))
tok_corpora<-sampled_files%>%
    unnest_tokens(word,text)%>%
    anti_join(stop_words)
tok_table<-tok_corpora%>%
    count(from,word,sort=TRUE,name = "word_count")
tok_table%>%
    group_by(from)%>%
    arrange(word_count)%>%
  knitr::kable()
```

After analyzing im getting to the point that the wierder words are really low frequency. It will take longer to remove them than to simply go ahead and do frequency analysis and see if the meaningful most frequent words arent garbage.

The only thing I will apply is this previous filter plus the one from the top10 analysis and save it to the tok_corpora:

```{r}
tok_corpora<-tok_corpora%>%
  filter(!str_detect(word,"^[0-9]+$|^it’s$|^don’t$|^i’m$|rt|lol|^_+$|^[0-9\\._]{2,}$"))

tok_table<-tok_corpora%>%
    count(from,word,sort=TRUE,name = "word_count")
word_freq<-tok_table%>%
    group_by(from)%>%
    arrange(desc(word_count))
word_freq%>%
    knitr::kable()
```

Following Silge's book, im going to make a frequency plot of news versus twitter and blogs to see how frequencies fare between sources.

First a suitable dataset:

```{r}

word_freq_news_vs_other<-word_freq%>%
    group_by(from)%>%
    mutate(proportion=word_count/sum(word_count))%>%
    select(-word_count)%>%
    pivot_wider(names_from = from,values_from = proportion)%>%
    pivot_longer(c(twitter,blogs),names_to="origin",values_to="proportion")
knitr::kable(word_freq_news_vs_other%>%slice_sample(n = 10))
```

So there. Now the plot, simplified version:

```{r}
word_freq_news_vs_other%>%
    ggplot(aes(x=proportion,y=news))+
    geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
    geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5)+
    scale_color_gradient(limits = c(0, 0.001), 
                         low = "darkslategray4", high = "gray75") +
    facet_wrap(~origin,ncol=2)+
    theme(legend.position="none")+
    labs(y="News",x=NULL)
```

Well that does errr.... nothing for us. It is clear love is a popular word. It is the constant here. But nothing else.

We gotta n-grammify this thing cause its too big, babe. Or maybe split by alphabet and look at stuff, but that will be another rabbit hole.

```{r}

```
